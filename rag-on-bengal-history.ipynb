{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":28785,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: A Retreival Augmented Generation (RAG)-based AI assistant\n\n#### We use Retreival Augmented Generation (RAG) to answer questions on the document(s) using an LLM\n#### In this notebook we go through the following steps:\n\n1. Collect the document(s) from different webpages in the website: We choose to build an AI assistant on the history of Bengal, so we pick the relevant pages from wikipedia\n2. Split the document(s) into chunks -- could be the original pages themselves\n3. Transform the chunks into embedding vectors \n3. Create a FAISS vector database with the chunks and embeddings\n4. Query the database with a question, and retrieve the most relevant chunks, which we call context\n5. Combine a \"system prompt\" and the context to create a query for the LLM\n6. Generate an answer from the LLM using this query\n\n#### - Large Language Model used: Gemma2B \n#### - Embedding Model used: bge-small-en-v1.5\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Pip install packages ","metadata":{"execution":{"iopub.status.busy":"2024-03-28T17:05:09.237587Z","iopub.execute_input":"2024-03-28T17:05:09.237875Z","iopub.status.idle":"2024-03-28T17:05:09.241633Z","shell.execute_reply.started":"2024-03-28T17:05:09.237852Z","shell.execute_reply":"2024-03-28T17:05:09.240748Z"}}},{"cell_type":"code","source":"%%time\n%%capture\n!pip install tiktoken FlagEmbedding transformers faiss-gpu\n!pip install sentence_transformers\n!pip install -q -U wikipedia-api\n#!pip install requests bs4","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T19:47:04.518104Z","iopub.execute_input":"2024-07-19T19:47:04.518530Z","iopub.status.idle":"2024-07-19T19:47:57.966070Z","shell.execute_reply.started":"2024-07-19T19:47:04.518496Z","shell.execute_reply":"2024-07-19T19:47:57.964707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Obtain the wikipedia pages for a given topic","metadata":{"execution":{"iopub.status.busy":"2024-03-28T15:14:22.259948Z","iopub.execute_input":"2024-03-28T15:14:22.260257Z","iopub.status.idle":"2024-03-28T15:14:22.26454Z","shell.execute_reply.started":"2024-03-28T15:14:22.260229Z","shell.execute_reply":"2024-03-28T15:14:22.263646Z"}}},{"cell_type":"code","source":"from tqdm import tqdm\nimport wikipediaapi\nimport re\n\n# Pre-compile the regular expression pattern for better performance\nBRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n\ndef remove_braces_and_content(text):\n    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n    return BRACES_PATTERN.sub('', text)\n\ndef clean_string(input_string):\n    \"\"\"Clean the input string.\"\"\"\n    \n    # Remove extra spaces by splitting the string by spaces and joining back together\n    cleaned_string = ' '.join(input_string.split())\n    \n    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n    \n    # Remove all occurrences of curly braces and their content from the cleaned string\n    cleaned_string = remove_braces_and_content(cleaned_string)\n    \n    # Return the cleaned string\n    return cleaned_string\n\ndef extract_wiki(wiki, category_name):\n    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n    \n    # Get the Wikipedia page corresponding to the provided category name\n    category = wiki.page(\"Category:\" + category_name)\n    \n    # Initialize an empty list to store page titles\n    pages = []\n    \n    # Check if the category exists\n    if category.exists():\n        # Iterate through each article in the category and append its title to the list\n        for article in category.categorymembers.values():\n            pages.append(article.title)\n    \n    # Return the list of page titles\n    return pages\n\n\ndef get_wiki(categories):\n    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n    \n    # Create a Wikipedia object\n    wiki_wiki = wikipediaapi.Wikipedia('AI_Assistant', 'en')\n    \n    # Initialize lists to store explored categories and Wikipedia pages\n    explored_categories = []\n    wikipedia_pages = []\n\n    # Iterate through each category\n    print(\"- Processing Wikipedia categories:\")\n    for category_name in categories:\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Get the Wikipedia page corresponding to the category\n        category = wiki_wiki.page(\"Category:\" + category_name)\n        \n        # Extract Wikipedia pages from the category and extend the list\n        wikipedia_pages.extend(extract_wiki(wiki_wiki, category_name))\n        \n        # Add the explored category to the list\n        explored_categories.append(category_name)\n\n    # Extract subcategories and remove duplicate categories\n    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n    \n    # Explore subcategories recursively\n    while categories_to_explore:\n        category_name = categories_to_explore.pop()\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Extract more references from the subcategory\n        more_refs = extract_wiki(wiki_wiki, category_name)\n\n        # Iterate through the references\n        for ref in more_refs:\n            # Check if the reference is a category\n            if \"Category:\" in ref:\n                new_category = ref.replace(\"Category:\", \"\")\n                # Add the new category to the explored categories list\n                if new_category not in explored_categories:\n                    explored_categories.append(new_category)\n            else:\n                # Add the reference to the Wikipedia pages list\n                if ref not in wikipedia_pages:\n                    wikipedia_pages.append(ref)\n\n    # Initialize a list to store extracted texts\n    extracted_texts = []\n    \n    # Iterate through each Wikipedia page\n    print(\"- Processing Wikipedia pages:\")\n    for page_title in tqdm(wikipedia_pages):\n        try:\n            # Make a request to the Wikipedia page\n            page = wiki_wiki.page(page_title)\n\n            # Check if the page summary does not contain certain keywords\n            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n                # Append the page title and summary to the extracted texts list\n                if len(page.summary) > len(page.title):\n                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n\n                # Iterate through the sections in the page\n                for section in page.sections:\n                    # Append the page title and section text to the extracted texts list\n                    if len(section.text) > len(page.title):\n                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n                        \n        except Exception as e:\n            print(f\"Error processing page {page.title}: {e}\")\n                    \n    # Return the extracted texts\n    return extracted_texts","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:56:03.366189Z","iopub.execute_input":"2024-07-19T19:56:03.366570Z","iopub.status.idle":"2024-07-19T19:56:03.386356Z","shell.execute_reply.started":"2024-07-19T19:56:03.366542Z","shell.execute_reply":"2024-07-19T19:56:03.385240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# collect all the pages under these categories....\ncategories = [\"History_of_Bengal\", \"History of Bangladesh\", \"Kolkata\",\n             \"History of Kolkata\", \"History of West Bengal\" ]\nextracted_texts = get_wiki(categories)\nprint(\"Found\", len(extracted_texts), \"Wikipedia pages\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:00:31.219097Z","iopub.execute_input":"2024-07-19T20:00:31.220020Z","iopub.status.idle":"2024-07-19T20:06:04.449740Z","shell.execute_reply.started":"2024-07-19T20:00:31.219985Z","shell.execute_reply":"2024-07-19T20:06:04.448756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Dump the texts for future use \n\nimport pickle\nwith open(\"bengal_history_wikidump.pickle\", 'wb') as file:\n    pickle.dump(extracted_texts, file)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:13:28.525279Z","iopub.execute_input":"2024-07-19T20:13:28.525678Z","iopub.status.idle":"2024-07-19T20:13:28.551081Z","shell.execute_reply.started":"2024-07-19T20:13:28.525644Z","shell.execute_reply":"2024-07-19T20:13:28.549940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reformat text chunks","metadata":{}},{"cell_type":"markdown","source":"#### How large are the chunks on average?","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom matplotlib import pyplot as plt\n\nchunks_sizes = [len(chunk) for chunk in extracted_texts]\nprint(\"Average number of characters per chunk : \"\"%.2f\" % np.mean(chunks_sizes))\n\npercentile_50th = np.percentile(chunks_sizes, 50)\nplt.hist(chunks_sizes, bins=30)\nplt.title('Distribution of text-chunks length, with 50th percentile')\nplt.xlabel('Length of text-chunk in characters')\nplt.axvline(x = percentile_50th, color = 'yellow', linestyle = '--', alpha = 0.9)\nplt.gca().spines[['top', 'right',]].set_visible(False)\nplt.show();\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:39:12.387579Z","iopub.execute_input":"2024-07-19T20:39:12.388344Z","iopub.status.idle":"2024-07-19T20:39:12.647193Z","shell.execute_reply.started":"2024-07-19T20:39:12.388307Z","shell.execute_reply":"2024-07-19T20:39:12.646206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_text_chunks(strings, K, L):\n    result = []\n    for string in strings:\n        if len(string) > K:\n            i = 0\n            while i < len(string):\n                result.append(string[i:i+K])\n                i += K - L\n        else:\n            result.append(string)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:43:57.343337Z","iopub.execute_input":"2024-07-19T20:43:57.343960Z","iopub.status.idle":"2024-07-19T20:43:57.350120Z","shell.execute_reply.started":"2024-07-19T20:43:57.343924Z","shell.execute_reply":"2024-07-19T20:43:57.349024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_chunks = split_text_chunks(extracted_texts, K=2000, L =1000)\n\nchunks_sizes = [len(chunk) for chunk in new_chunks]\nprint(\"Average number of characters per chunk : \"\"%.2f\" % np.mean(chunks_sizes))\n\npercentile_50th = np.percentile(chunks_sizes, 50)\nplt.hist(chunks_sizes, bins=30)\nplt.title('Distribution of text-chunks length, with 50th percentile')\nplt.xlabel('Length of text-chunk in characters')\nplt.axvline(x = percentile_50th, color = 'yellow', linestyle = '--', alpha = 0.9)\nplt.gca().spines[['top', 'right',]].set_visible(False)\nplt.show();\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:43:57.907430Z","iopub.execute_input":"2024-07-19T20:43:57.908073Z","iopub.status.idle":"2024-07-19T20:43:58.236424Z","shell.execute_reply.started":"2024-07-19T20:43:57.908037Z","shell.execute_reply":"2024-07-19T20:43:58.235231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(new_chunks)\n#len(extracted_texts)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:44:05.158299Z","iopub.execute_input":"2024-07-19T20:44:05.159109Z","iopub.status.idle":"2024-07-19T20:44:05.165229Z","shell.execute_reply.started":"2024-07-19T20:44:05.159071Z","shell.execute_reply":"2024-07-19T20:44:05.164156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build a vector database using an embedding model","metadata":{}},{"cell_type":"markdown","source":"\n#### Download an embedding model. We consider 'BAAI/bge-small-en-v1.5' \n- The Embedding model will transform the text chunks into numerical vectors in high-dimensions\n- Embedding vectors corresponding to text chunks similar in meaning (semantics) to each other, are closer to each other in the vector space\n- Next we use FAISS to build a vector database","metadata":{}},{"cell_type":"code","source":"%%time\n# Load a pre-trained sentence transformer model\nfrom sentence_transformers import SentenceTransformer, util\nimport faiss\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#model = SentenceTransformer('all-MiniLM-L6-v2')\nmodel = SentenceTransformer('BAAI/bge-small-en-v1.5') \n                            \n# Function to embed text chunks\ndef embed_text_chunks(text_chunks, model):\n    embeddings = model.encode(text_chunks, convert_to_tensor=True, device='cuda')\n    return embeddings.cpu().numpy()\n\n# Function to create FAISS index\ndef create_faiss_index(text_chunks, model):\n    embeddings = embed_text_chunks(text_chunks, model)\n    dimension = embeddings.shape[1]\n    index = faiss.IndexFlatL2(dimension)\n    index.add(embeddings)\n    del embeddings\n    return index\n\n# Create the FAISS index\nfaiss_index = create_faiss_index(new_chunks, model)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T22:00:18.214358Z","iopub.execute_input":"2024-07-19T22:00:18.214788Z","iopub.status.idle":"2024-07-19T22:00:58.680500Z","shell.execute_reply.started":"2024-07-19T22:00:18.214752Z","shell.execute_reply":"2024-07-19T22:00:58.679418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T22:02:45.590370Z","iopub.execute_input":"2024-07-19T22:02:45.590777Z","iopub.status.idle":"2024-07-19T22:02:45.596105Z","shell.execute_reply.started":"2024-07-19T22:02:45.590749Z","shell.execute_reply":"2024-07-19T22:02:45.594861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Download the reranker model.  Consider 'BAAI/bge-reranker-large'\n- A query on the embedding vectors returns chunks that are most relevant to the query\n- A reranker is used to further evaluate these chunks, based on certain criteria \n- It reorders or \"re-ranks\" them according to their quality or relevance","metadata":{}},{"cell_type":"code","source":"%%time\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:56:21.108344Z","iopub.execute_input":"2024-07-19T20:56:21.109290Z","iopub.status.idle":"2024-07-19T20:57:01.888154Z","shell.execute_reply.started":"2024-07-19T20:56:21.109250Z","shell.execute_reply":"2024-07-19T20:57:01.887064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to search the FAISS index\ndef query_faiss(query_text, index, model, k=5):\n    query_embedding = model.encode([query_text], convert_to_tensor=True, device='cuda').cpu().numpy()\n    distances, indices = index.search(query_embedding, k)\n    return distances, indices\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:05:57.742088Z","iopub.execute_input":"2024-07-19T21:05:57.742973Z","iopub.status.idle":"2024-07-19T21:05:57.748721Z","shell.execute_reply.started":"2024-07-19T21:05:57.742937Z","shell.execute_reply":"2024-07-19T21:05:57.747600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Large Language Model and Query\n\n- We use Gemma2B, a pretrained lightweight LLM wih 2 billion parameters, released by Google \n","metadata":{}},{"cell_type":"code","source":"%%time\n\nlanguage_model_name = \"/kaggle/input/gemma/transformers/2b-it/3\" #\"\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(language_model_name, use_fast = True)\nlanguage_model = AutoModelForCausalLM.from_pretrained(language_model_name, \n                            device_map=\"cuda\",  # change this to \"auto\" if you want to run the LLM distributed on 2 GPUs, \n                             trust_remote_code=False,\n                                             revision=\"main\")\nlanguage_model.config.hidden_activation = 'gelu_pytorch_tanh'\n\n\n### insert your code here!\nprint('Done loading model: '+ language_model_name)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:02:08.888453Z","iopub.execute_input":"2024-07-19T21:02:08.889247Z","iopub.status.idle":"2024-07-19T21:02:44.312715Z","shell.execute_reply.started":"2024-07-19T21:02:08.889205Z","shell.execute_reply":"2024-07-19T21:02:44.311765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Specify system prompt\n","metadata":{}},{"cell_type":"code","source":"system_message =  \"\"\"You are a Professor of Bengali History with great expertise in the history of Bengal. \nAnswer the question using the context provided.\nAnswer must be very detailed and thorough.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-19T22:08:05.181704Z","iopub.execute_input":"2024-07-19T22:08:05.182085Z","iopub.status.idle":"2024-07-19T22:08:05.187565Z","shell.execute_reply.started":"2024-07-19T22:08:05.182057Z","shell.execute_reply":"2024-07-19T22:08:05.186384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Specify the query, aka question we want to ask\n- Some example questions\n","metadata":{}},{"cell_type":"code","source":"\nquery1 = \"Who was Shashanka?\" \nquery2 = \"When did the partition of Bengal take place?\" \nquery3 = \"Who was Netaji Subhash Chandra Bose?\" \nquery4 = \"When was Mujibur Rahaman assassinated?\" \nquery5 = \"What is Rabindranath Tagore famous for?\"\nquery6 = \"Where was the Portuguese trading posts in Bengal?\"\nquery7 = \"Who founded the Pala Empire?\" \nquery8 = \"What was thw religion of the Pala Empire?\"\nquery9 =  \"How did Islam spread in Bengal?\"","metadata":{"execution":{"iopub.status.busy":"2024-07-19T22:19:02.924283Z","iopub.execute_input":"2024-07-19T22:19:02.924711Z","iopub.status.idle":"2024-07-19T22:19:02.931281Z","shell.execute_reply.started":"2024-07-19T22:19:02.924677Z","shell.execute_reply":"2024-07-19T22:19:02.929796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### query the vector database to identify matching chunks\n\n- We choose the top 5 best matching chunks (this number could be something else as well, by varying the n_results parameter)\n\n#### context_chunks_init are the chunks with relevant context (with regards to the query)\n\n- We will use the reranker to sort these chunks based on relevance\n\n#### Keep the \"best\" chunks that are the most relevant as per the reranker\n - Use a threshold\n \n #### Query the LLM with the full query, with system_message, context and query","metadata":{}},{"cell_type":"code","source":"%%time\nn_top_matches = 5\n\nquery = query9\ndistances, indices = query_faiss(query, faiss_index, model, k = n_top_matches)\nprint(distances)\ncontext_chunks_init = []\nfor i in range(n_top_matches):\n    context_chunks_init.append(new_chunks[indices[0][i]])\n    \n#make pairs of query and chunks\nquery_and_chunks = [[query, chunk] for chunk in context_chunks_init]\nscores_reranker = reranker.compute_score(query_and_chunks)\nprint(\"Scores of retrieved chunks:\\n\", np.round(  scores_reranker, decimals =2))\n# indexes sorted according to new rank\nprint(\"\\nOld order of retrieved chunks:\", np.arange(n_top_matches))\nmax_idx_reranked = np.argsort(-np.array(scores_reranker))\nprint(\"\\nRe-ranked order of retrieved chunks:\", max_idx_reranked)\n\ncut_off_score = 3\nmin_score = np.max(scores_reranker)\n# if np.max(scores_reranker) > cut_off_score:\n#     cut_off_score = 3\n\ncontext_chunks = []\nfor idx in max_idx_reranked:\n    if scores_reranker[idx] >= min_score:\n        context_chunks.append(context_chunks_init[idx])\n        \nprompt_template = system_message + \"\"\"\\n\\n\\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\ncontext_chunks_as_str = '\\n###\\n'.join([str(elem) for elem in context_chunks])\nllm_full_query = prompt_template.format(context=context_chunks_as_str, question=query)\n\n\ninput_ids = tokenizer(llm_full_query, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = language_model.generate(**input_ids, max_new_tokens = 1024, do_sample = True, top_k = 5, top_p = 0.1, temperature = 0.1)\nfull_answer = tokenizer.decode(outputs[0])\n\nprinted_answer = full_answer.split(\"Answer:\")[1]\nprinted_answer = printed_answer.split(\"<eos>\")[0]\nprint(\"\\nQuestion:\" + query+\"\\n\")\nprint(printed_answer)\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T22:19:22.343056Z","iopub.execute_input":"2024-07-19T22:19:22.343487Z","iopub.status.idle":"2024-07-19T22:19:24.172676Z","shell.execute_reply.started":"2024-07-19T22:19:22.343451Z","shell.execute_reply":"2024-07-19T22:19:24.171567Z"},"trusted":true},"execution_count":null,"outputs":[]}]}